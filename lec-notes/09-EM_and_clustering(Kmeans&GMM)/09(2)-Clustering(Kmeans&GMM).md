æ— ç›‘ç£å­¦ä¹ ï¼ˆ**Unsupervised Learning**ï¼‰æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ç§ç±»å‹ï¼Œå®ƒçš„æ ¸å¿ƒç‰¹ç‚¹æ˜¯ï¼š**è®­ç»ƒæ•°æ®æ²¡æœ‰æ ‡ç­¾**ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œç®—æ³•åœ¨å­¦ä¹ æ—¶ï¼Œå¹¶ä¸çŸ¥é“å“ªäº›æ˜¯â€œå¯¹â€æˆ–è€…â€œé”™â€çš„ç­”æ¡ˆï¼Œå®ƒåªèƒ½ä»æ•°æ®ä¸­è‡ªå·±â€œæ‰¾è§„å¾‹â€

Clustering é—®é¢˜æ˜¯ä¸€ç±» unsupervised learning é—®é¢˜.

example: customer segmentation. å°†å®¢æˆ·åˆ†ç»„ï¼Œä»¥ä¾¿åœ¨å†³ç­–ï¼ˆå¦‚ä¿¡ç”¨å¡å®¡æ‰¹ï¼‰æˆ–è¥é”€ï¼ˆå¦‚äº§å“æ¨å¹¿ï¼‰ä¸­æä¾›å¸®åŠ©ã€‚

æˆ‘ä»¬å°†ä»‹ç»ä¸¤ç§ clustering çš„ç®—æ³•: K-means ä»¥åŠ GMM

K-means å…¶å®ç®—æ˜¯ GMM çš„ä¸€ç§ç‰¹æ®Šæƒ…å†µ.



# K-means

k-means Algorithm çš„ idea å³: 

- ç»™å®š **æ— æ ‡ç­¾** æ•°æ®  
  $\{ \mathbf{x}^{(n)} \} \quad (n = 1, \ldots, N)$
- **å‡è®¾è¿™äº›æ•°æ®å±äº $K$ ä¸ª clusters**ï¼ˆex: $K = 2$ï¼‰
- æ‰¾åˆ°è¿™äº›ç°‡



ä½¿ç”¨æŒ‡ç¤ºå˜é‡ $r_{nk} \in \{0, 1\}, 1\leq n \leq N,1\leq k \leq K$ ä½œä¸ºå¾…å­¦ä¹ çš„ parameterï¼š

- $r_{nk} = 1$ å½“ $\mathbf{x}^{(n)}$ å±äºç¬¬ $k$ ä¸ªç°‡æ—¶

- $r_{nk} = 0$ otherwise





å¯»æ‰¾ cluster center $\mu_k$ å’Œåˆ†é…å˜é‡ $r_{nk}$ï¼Œä»¥ minimize **distortion measure $J$**ï¼š
$$
J = \sum_{k=1}^K \sum_{n=1}^N r_{nk} \left\| \mathbf{x}^{(n)} - \mu_k \right\|^2
$$

å…¶ä¸­ cluster center çš„è®¡ç®—å…¬å¼ä¸ºï¼š
$$
\mu_k = \frac{1}{N_k} \sum_{n : \mathbf{x}^{(n)} \in \text{cluster } k} \mathbf{x}^{(n)} = \frac{\sum_{n=1}^N r_{nk} \mathbf{x}^{(n)}}{\sum_{n=1}^N r_{nk}}
$$
(å½“ç„¶è¿™æ˜¯ trivial çš„, å°±æ˜¯ mean point)

distortion measure $J$ å°±æ˜¯: squared distance of points from the center of its own cluster, ç®€ç§°ä¸º intra-cluster variation.



## The K-Means Algorithm

- åˆå§‹åŒ–ç°‡ä¸­å¿ƒ

- é‡å¤ä»¥ä¸‹æ­¥éª¤ç›´åˆ°æ”¶æ•›ï¼š

  - cluster assignment:ï¼ˆE stepï¼‰

    å°†æ¯ä¸ªç‚¹åˆ†é…ç»™æœ€è¿‘çš„ç°‡ä¸­å¿ƒï¼š
    $$
    r_{nk} =
    \begin{cases}
    1 & \text{è‹¥ } k = \arg\min_j \left\| \mathbf{x}^{(n)} - \mu_j \right\|^2 \\
    0 & \text{å¦åˆ™}
    \end{cases}
    $$

  - å‚æ•°æ›´æ–°ï¼šæ›´æ–°ç°‡ä¸­å¿ƒï¼ˆM stepï¼‰

  $$
  \mu_k = \frac{\sum_n r_{nk} \mathbf{x}^{(n)}}{\sum_n r_{nk}}
  $$







```python
import numpy as np

def euclidean_distance(x: np.ndarray, y: np.ndarray) -> np.ndarray:
    assert x.shape == y.shape
    error = np.sqrt(np.sum(np.power(x - y, 2), axis=-1))
    return error

def train_kmeans(train_data: np.ndarray, initial_centroids, *, num_iterations: int = 50):
    N, d = train_data.shape
    K, d2 = initial_centroids.shape
    if d != d2:
        raise ValueError(f"Invalid dimension: {d} != {d2}")
    assert train_data.dtype.kind == 'f'

    centroids = initial_centroids.copy()
    
    for i in range(num_iterations):
        # E-step: Assign each point to the nearest centroid
        # Expand dims for broadcasting: train_data (N, 1, d), centroids (1, K, d)
        distances = np.linalg.norm(train_data[:, np.newaxis, :] - centroids[np.newaxis, :, :], axis=2)  # shape (N, K)
        labels = np.argmin(distances, axis=1)  # shape (N,)

        # M-step: Update centroids
        new_centroids = np.zeros_like(centroids)
        for k in range(K):
            points_in_cluster = train_data[labels == k]
            if len(points_in_cluster) > 0:
                new_centroids[k] = np.mean(points_in_cluster, axis=0)
            else:
                new_centroids[k] = centroids[k]  # If a cluster has no points, keep old centroid

        centroids = new_centroids
        
        # monitor convergence
        assigned_centroids = centroids[labels]
        mean_error = np.mean(euclidean_distance(train_data, assigned_centroids))
        print(f'Iteration {i:2d}: mean error = {mean_error:2.2f}')
        
    assert centroids.shape == (K, d)
    return centroids
```





### Applying K-means to image compression

```python
import numpy as np

def compress_image(image: np.ndarray, centroids: np.ndarray) -> np.ndarray:
    """Compress image by mapping each pixel to the closest centroid."""

    H, W, C = image.shape
    K, C2 = centroids.shape
    assert C == C2 == 3, "Invalid number of channels."
    assert image.dtype == np.uint8

    # Step 1: reshape image to (N, 3) where N = H * W
    flat_image = image.reshape(-1, 3).astype(np.float32)  # shape (N, 3)

    # Step 2: compute distances to all centroids
    distances = np.linalg.norm(flat_image[:, np.newaxis, :] - centroids[np.newaxis, :, :], axis=2)  # shape (N, K)

    # Step 3: find closest centroid index for each pixel
    labels = np.argmin(distances, axis=1)  # shape (N,)

    # Step 4: map each pixel to its centroid
    compressed_flat = centroids[labels]  # shape (N, 3)

    # Step 5: reshape back to image shape
    compressed_image = np.round(compressed_flat).astype(np.uint8).reshape(H, W, C)

    assert compressed_image.dtype == np.uint8
    assert compressed_image.shape == (H, W, C)
    return compressed_image

```

Note: è¿™ä¸ª imageï¼Œæˆ‘ä»¬åœ¨å¤„ç†çš„æ—¶å€™éƒ½æ˜¯å˜æˆ (N = HM, 3) æ¥å¤„ç†

**å¦‚æœæˆ‘ä»¬æŠŠå›¾åƒ reshape æˆ (N, 3)ï¼Œæ˜¯ä¸æ˜¯å°±ä¸¢æ‰äº†åƒç´ ä¹‹é—´çš„ç©ºé—´ç»“æ„ï¼ˆspace topologyï¼‰ï¼Ÿæ¯”å¦‚è°æ˜¯é‚»å±…ï¼Œå“ªä¸ªåŒºåŸŸå±äºåŒä¸€ç‰©ä½“ï¼Ÿ**

ç­”æ¡ˆæ˜¯ï¼š**æ˜¯çš„ï¼Œæ ‡å‡†çš„ K-means æ˜¯å¿½ç•¥ç©ºé—´æ‹“æ‰‘ç»“æ„çš„ï¼**



ğŸ“¦ ä¸ºä»€ä¹ˆæˆ‘ä»¬è¿˜èƒ½ç”¨å®ƒæ¥å‹ç¼©å›¾åƒï¼Ÿ

è™½ç„¶ K-means å¿½ç•¥ç©ºé—´å…³ç³»ï¼Œå®ƒä»ç„¶èƒ½åœ¨ **é¢œè‰²åˆ†å¸ƒ** ä¸Šåšèšç±» â€”â€” ä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒè¯•å›¾æŠŠæ‰€æœ‰åƒç´ çš„é¢œè‰²å‹ç¼©æˆ K ç§ä»£è¡¨è‰²ã€‚

å®ƒçš„æ•ˆæœå¥½ä¸å¥½ï¼Œ**å–å†³äºå›¾åƒæ˜¯â€œè‰²å½©åˆ†å¸ƒä¸»å¯¼â€è¿˜æ˜¯â€œç©ºé—´ç»“æ„ä¸»å¯¼â€**ï¼š

| å›¾åƒç±»å‹                         | K-means å‹ç¼©æ•ˆæœ               |
| -------------------------------- | ------------------------------ |
| æ¸å˜ã€çº¯è‰²å—ã€å¤šå½©å¡é€šå›¾         | å¾ˆå¥½                           |
| çº¹ç†å¤æ‚ã€ç»“æ„å¼ºçƒˆã€è¾¹ç¼˜æ¸…æ™°çš„å›¾ | å®¹æ˜“å‡ºç° artifactsï¼ˆé©¬èµ›å…‹æ„Ÿï¼‰ |

------

## ğŸ¤” é‚£å¦‚æœæˆ‘æƒ³è¦ç”¨ç©ºé—´ç»“æ„å‘¢ï¼Ÿ

è¿™å°±å¼•å‡ºäº†æ›´é«˜çº§çš„æ¨¡å‹ï¼Œæ¯”å¦‚ï¼š

### 1. **Spatial-aware K-means / Bilateral K-means**

- ä½ å¯ä»¥æŠŠæ¯ä¸ªåƒç´ é™¤äº† `(R, G, B)` ä¹‹å¤–ï¼Œå†åŠ ä¸Š `(x, y)` ç©ºé—´åæ ‡ï¼
- å˜æˆ `(R, G, B, x, y)`ï¼Œä¹Ÿå°±æ˜¯ `d = 5` çš„æ•°æ®ç‚¹ã€‚
- ç„¶ååš K-meansï¼Œå°±èƒ½åŒæ—¶è€ƒè™‘é¢œè‰² + ç©ºé—´é‚»è¿‘æ€§ã€‚

ç¤ºæ„ä»£ç ï¼š

```python
H, W, _ = image.shape
pixels = image.reshape(-1, 3).astype(np.float32)

# åŠ å…¥ç©ºé—´ä½ç½®ï¼ˆå½’ä¸€åŒ–ï¼‰
x_coords, y_coords = np.meshgrid(np.linspace(0, 1, W), np.linspace(0, 1, H))
position = np.stack([x_coords, y_coords], axis=2).reshape(-1, 2)

# æ‹¼æ¥é¢œè‰²å’Œä½ç½®
features = np.concatenate([pixels, position], axis=1)  # shape (N, 5)

# è®­ç»ƒ K-means
centroids = train_kmeans(features, initial_centroids=... , num_iterations=...)
```

å½“ç„¶è¿™æ ·å‹ç¼©å‡ºæ¥çš„é¢œè‰²å¯èƒ½è¦å†ä» `centroids[:, :3]` æ‹¿å›æ¥å˜æˆå›¾åƒã€‚

------

### 2. **Graph-based clustering**

å¦‚æœä½ çœŸçš„å¾ˆå…³å¿ƒâ€œè°è·Ÿè°æ˜¯é‚»å±…â€ï¼Œå¯ä»¥ç”¨ï¼š

- Mean shift
- DBSCAN
- æˆ–è€…æ„å›¾ç„¶åç”¨ spectral clustering

è¿™äº›éƒ½æ›´å°Šé‡ç©ºé—´æ‹“æ‰‘ï¼Œä½†è®¡ç®—ä»£ä»·ä¹Ÿæ›´é«˜ã€‚

------

## âœ… å°ç»“

| æ–¹æ³•                          | æ˜¯å¦è€ƒè™‘ç©ºé—´ç»“æ„ | è¯´æ˜                               |
| ----------------------------- | ---------------- | ---------------------------------- |
| åŸå§‹ K-means                  | âŒ                | åªè€ƒè™‘é¢œè‰²ï¼Œé€Ÿåº¦å¿«ï¼Œç»“æœåƒè‰²å½©é‡åŒ– |
| K-means + åæ ‡                | âœ…ï¼ˆéƒ¨åˆ†ï¼‰        | æŠ˜ä¸­æ–¹æ³•ï¼Œè€ƒè™‘é¢œè‰² + ç©ºé—´          |
| å›¾åƒåˆ†å‰²ï¼ˆå¦‚ SLIC, GraphCutï¼‰ | âœ…âœ…               | æ›´å¤æ‚ï¼Œæ›´ä¿æ‹“æ‰‘ï¼Œä½†æ›´æ…¢           |

------

ä½ å¯¹ â€œspace topologyâ€ æ„Ÿå…´è¶£æ˜¯å¾ˆæ£’çš„æ–¹å‘â€”â€”è¿™æ˜¯è®¡ç®—æœºè§†è§‰é‡Œå›¾åƒåˆ†å‰²ã€è¯­ä¹‰åˆ†å‰²çš„é‡è¦è¯é¢˜ã€‚å¦‚æœä½ æ„¿æ„æ·±å…¥ï¼Œæˆ‘å¯ä»¥ç»™ä½ æ¨èä¸€äº›å…¥é—¨ä¾‹å­æˆ–è®ºæ–‡ã€‚

è¦ä¸è¦æˆ‘ç»™ä½ åŠ ä¸Šç©ºé—´æ„ŸçŸ¥çš„ K-means ç‰ˆæœ¬ï¼Ÿ





# Gaussian Mixture 





**K-Means ä½¿ç”¨ hard clustering assignment**

- æ¯ä¸ªç‚¹**åªèƒ½**å±äºä¸€ä¸ªç°‡ã€‚



**é«˜æ–¯æ··åˆæ¨¡å‹ï¼ˆMixture of Gaussiansï¼‰ä½¿ç”¨ soft clustering**

- ä¸€ä¸ªç‚¹å¯ä»¥è¢«å¤šä¸ªç°‡å…±åŒè§£é‡Šã€‚
- ä¸åŒçš„ç°‡ä¼šæ‰¿æ‹…ä¸åŒç¨‹åº¦çš„â€œè´£ä»»â€ï¼ˆresponsibilityï¼‰ï¼Œå³è¯¥ç‚¹å±äºæ¯ä¸ªç°‡çš„åéªŒæ¦‚ç‡ã€‚
- ï¼ˆå®é™…ä¸Šï¼Œè¯¥ç‚¹åªç”±æŸä¸€ä¸ªç°‡ç”Ÿæˆï¼Œä½†æˆ‘ä»¬ä¸çŸ¥é“å…·ä½“æ˜¯å“ªä¸ªï¼Œäºæ˜¯æˆ‘ä»¬ä¸ºæ¯ä¸ªç°‡åˆ†é…ä¸€ä¸ªæ¦‚ç‡ï¼‰



**å›¾ç¤ºï¼š**å¦‚ $(0.97, 0.03)$ è¡¨ç¤ºè¯¥ç‚¹ä»¥ 97% æ¦‚ç‡å±äº ç°‡2ï¼Œä»¥ 3% æ¦‚ç‡å±äºç°‡2

![Screenshot 2025-03-31 at 15.37.07](09(2)-Clustering(Kmeans&GMM).assets/Screenshot 2025-03-31 at 15.37.07.png)

ä¸åŒçš„ clusters take different levels of responsibility for a point. ä¸€ä¸ªç‚¹ä¸Š, responsibility å³ posterior probability.





## Modeling











## GMM Algorithm



> - Initialize parameters randomly $\theta=\left\{\pi_k, \mu_k, \Sigma_k\right\}_{k=1}^K$
>
> - Repeat until convergence (alternating optimization)
>
> - E Step: Given fixed parameters $\theta$, set $q^{(n)}(\mathbf{z})=p\left(\mathbf{z} \mid \mathbf{x}^{(n)}, \theta\right)$
>   $$
>   \gamma\left(z_{n k}\right)=\frac{\pi_k \mathcal{N}\left(\mathbf{x}^{(n)} \mid \mu_k, \Sigma_k\right)}{\sum_{j=1}^K \pi_j \mathcal{N}\left(\mathbf{x}^{(n)} \mid \mu_j, \Sigma_j\right)}=P\left(z_k=1 \mid \mathbf{x}^{(n)}\right)
>   $$
>   
>
> - M Step: Given fixed $q\left(\mathbf{z}^{(n)}\right)^{\prime}$ 's for $\mathbf{x}^{(n)}$ 's (or $\left.\gamma\left(z_{n k}\right)\right)$, update $\theta$ :
>
>   in order to get 
>   $$
>   \theta^{new} : = \arg \max _\theta \sum_n \sum_{\mathbf{z}} q^{(n)}(\mathbf{z}) \log p\left(\mathbf{z}, \mathbf{x}^{(n)} \mid \theta\right)
>   $$
>   We update:
>   $$
>   \begin{aligned}
>   & \pi_k^{\text {new }}=\frac{N_k}{N}=\frac{\sum_n \gamma\left(z_{n k}\right)}{N} \\
>   & \mu_k^{\text {new }}=\frac{1}{N_k} \sum_{n=1}^N \gamma\left(z_{n k}\right) \mathbf{x}^{(n)} \\
>   & \Sigma_k^{\text {new }}=\frac{1}{N_k} \sum_{n=1}^N \gamma\left(z_{n k}\right)\left(\mathbf{x}^{(n)}-\mu_k^{\text {new }}\right)\left(\mathbf{x}^{(n)}-\mu_k^{\text {new }}\right)^{\top}
>   \end{aligned}
>   $$
>



### Proof of M step update rule

æˆ‘ä»¬ä¸Šé¢å·²ç»ç»™å‡º parameter çš„æ›´æ–°ç®—æ³•ï¼Œç°åœ¨æˆ‘ä»¬è¯æ˜ä¸ºä»€ä¹ˆè¿™äº› Parameters æ˜¯ argmax çš„.

é¦–å…ˆï¼Œæˆ‘ä»¬å…ˆç®€åŒ–è¡¨è¾¾å¼ï¼š
$$
\begin{aligned}
J(\boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma})= & \sum_{n=1}^N \sum_{k=1}^K q^{(n)}\left(\mathbf{z}_k\right) \log p\left(\mathbf{z}_k, \mathbf{x}^{(n)} \mid \pi_k, \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k\right) \\
= & \sum_{n=1}^N \sum_{k=1}^K \gamma\left(\mathbf{z}_{n k}\right)\left(\log \pi_k+\log \frac{1}{(2 \pi)^{m / 2}\left(\operatorname{det} \boldsymbol{\Sigma}_k\right)^{1 / 2}}-\frac{1}{2}\left(\mathbf{x}^{(n)}-\boldsymbol{\mu}_k\right)^{\top} \boldsymbol{\Sigma}_k^{-1}\left(\mathbf{x}^{(n)}-\boldsymbol{\mu}_k\right)\right. \bigg)\\
= & \sum_{n=1}^N \sum_{k=1}^K \gamma\left(\mathbf{z}_{n k}\right) \log \pi_k-\sum_{n=1}^N \sum_{k=1}^K \gamma\left(\mathbf{z}_{n k}\right) \log \left((2 \pi)^{m / 2}\left(\operatorname{det} \boldsymbol{\Sigma}_k\right)^{1 / 2}\right) \\
= & \sum_{n=1}^N \sum_{k=1}^K \gamma\left(\mathbf{z}_{n k}\right) \log \pi_k-\frac{1}{2} \sum_{n=1}^N \sum_{k=1}^K \gamma\left(\mathbf{z}_{n k}\right)\left(\mathbf{x}^{(n)}-\boldsymbol{\mu}_k\right)^{\top} \boldsymbol{\Sigma}_k^{-1}\left(\mathbf{x}^{(n)}-\boldsymbol{\mu}_k\right) \\
& -\frac{1}{2} \sum_{n=1}^N \sum_{k=1}^K \gamma\left(\mathbf{z}_{n k}\right)\left(\mathbf{x}^{(n)}-\boldsymbol{\mu}_k\right)^{\top} \boldsymbol{\Sigma}_k^{-1}\left(\mathbf{x}^{(n)}-\boldsymbol{\mu}_k\right)+\mathrm{const}
\end{aligned}
$$
å› è€Œæˆ‘ä»¬ differentiate:

















![{A4E22888-7017-4186-8D9C-B712353A6389}](09(2)-Clustering(Kmeans&GMM).assets/GMM.png)











# 2. [20 åˆ†] EM for GDA with missing labels

åœ¨æœ¬é¢˜ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ EM ç®—æ³•æ¥å¤„ç†æ ‡ç­¾ä¸å®Œæ•´æ—¶çš„é«˜æ–¯åˆ¤åˆ«åˆ†æ (GDA) é—®é¢˜

å‡è®¾ä½ æœ‰ä¸€ä¸ªæ•°æ®é›†ï¼Œå…¶ä¸­ä¸€éƒ¨åˆ†æ•°æ® is labeledï¼Œå¦ä¸€éƒ¨åˆ† unlabeled. 

æˆ‘ä»¬å¸Œæœ›åœ¨è¿™ä¸ª**partially labelled dataset**ä¸Šå­¦ä¹ ä¸€ä¸ª**generative model**. 

> æ³¨ï¼šè¿™ç§å­¦ä¹ è®¾å®šè¢«ç§°ä¸º**åŠç›‘ç£å­¦ä¹ ï¼ˆsemi-supervised learningï¼‰**ï¼Œæ˜¯æœºå™¨å­¦ä¹ ä¸­çš„ä¸€ä¸ªé‡è¦ç ”ç©¶æ–¹å‘ã€‚

In particular, æˆ‘ä»¬å‡è®¾æˆ‘ä»¬æœ‰ $l$ ä¸ªå¸¦æ ‡ç­¾çš„æ ·æœ¬å’Œ $u$ ä¸ªæœªæ ‡è®°çš„æ ·æœ¬ï¼Œå³ï¼š
$$
D = \{(x^{(1)}, y^{(1)}), \cdots, (x^{(l)}, y^{(l)}), x^{(l+1)}, \cdots, x^{(l+u)}\} \notag
$$

We also make the following assumptions

- The data is real-valued $M$ ç»´å‘é‡ï¼Œå³ $x \in \mathbb{R}^M$
- æ ‡ç­¾ $y \in \{0, 1\}$ï¼ˆå³ä¸€ä¸ªäºŒåˆ†ç±»é—®é¢˜ï¼‰
- We model the data following the same assumption as in GDA:

$$
P(x, y) = P(y) P(x \mid y) \tag{1}
$$

$$
P(y = j) =
\begin{cases}
\phi & \text{if } j = 1 \\
1 - \phi & \text{if } j = 0
\end{cases} \tag{2}
$$

$$
P(x \mid y = j) = \mathcal{N}(x; \mu_j, \Sigma_j), \quad j = 0 \text{ or } 1 \tag{3}
$$

where $\phi$ æ˜¯ä¼¯åŠªåˆ©åˆ†å¸ƒçš„å‚æ•°ï¼ˆå³ $0 \leq \phi \leq 1$ï¼‰ï¼Œ$\mu_j$ å’Œ $\Sigma_j$ åˆ†åˆ«æ˜¯ç¬¬ $j$ ç±»çš„ class-specific mean å’Œ covariance matrix. (å› ä¸ºåªæœ‰ä¸¤ç±», ä¸ºäº†ç®€åŒ–è®°å·ï¼Œä¹Ÿå¯ä»¥å†™æˆ $\phi_1 = \phi$ï¼Œ$\phi_0 = 1 - \phi$)

Multivariate Gaussian distribution $\mathcal{N}(x; \mu_j, \Sigma_j)$ å®šä¹‰å¦‚ä¸‹ï¼š

$$
p(x \mid y = j; \mu_j, \Sigma_j) = \mathcal{N}(x; \mu_j, \Sigma_j) =
\frac{1}{(2\pi)^{M/2} |\Sigma_j|^{1/2}} \exp\left(-\frac{1}{2}(x - \mu_j)^\top \Sigma_j^{-1}(x - \mu_j)\right) \tag{4}
$$

---

ç”±äºå­˜åœ¨æœªæ ‡è®°æ•°æ®ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æœ€å¤§åŒ–ä»¥ä¸‹æ··åˆç›®æ ‡å‡½æ•°ï¼š

$$
J = \sum_{i=1}^l \log p(x^{(i)}, y^{(i)}) + \lambda \sum_{i = l+1}^{l+u} \log p(x^{(i)}) \tag{5}
$$

å…¶ä¸­ï¼Œ$\lambda$ æ˜¯è¶…å‚æ•°ï¼Œç”¨äºæ§åˆ¶ labeled and unlabeled data çš„ weight.

ç”±äºæˆ‘ä»¬æ²¡æœ‰ explicitly model $p(x)$ çš„å½¢å¼ï¼Œæˆ‘ä»¬ä½¿ç”¨å…¨æ¦‚ç‡å…¬å¼å°†ç›®æ ‡å‡½æ•°é‡å†™ä¸ºï¼š

$$
J = \sum_{i=1}^l \log p(x^{(i)}, y^{(i)}) + \lambda \sum_{i = l+1}^{l+u} \log \sum_{j \in \{0,1\}} p(x^{(i)}, y^{(i)} = j) \tag{6}
$$

é€šè¿‡è¿™ç§æ–¹å¼ï¼Œunlabeled training examples ä½¿ç”¨äº†ä¸ labeled samples ç›¸åŒçš„æ¨¡å‹. æˆ‘ä»¬å°†ä½¿ç”¨ EM ç®—æ³•æ¥ä¼˜åŒ–ä¸Šè¿°ç›®æ ‡å‡½æ•°.

---

åœ¨æ¨å¯¼è§£æ³•æ—¶ï¼Œè¯·ç»™å‡ºæ‰€æœ‰å¿…è¦çš„æ¨å¯¼æ­¥éª¤ï¼Œå¹¶å°½é‡è§£é‡Šæ¨å¯¼çš„è¿‡ç¨‹ï¼Œä½¿ä¹‹æ˜“äºç†è§£ã€‚

> **æç¤ºï¼š** ä½ å¯ä»¥ä½¿ç”¨å¦‚ä¸‹ç­‰å¼ï¼š

$$
p(x^{(i)}, y^{(i)}) = \prod_{j \in \{0,1\}} \left[ \frac{\phi_j}{(2\pi)^{M/2} |\Sigma_j|^{1/2}} \exp\left( -\frac{1}{2}(x^{(i)} - \mu_j)^\top \Sigma_j^{-1}(x^{(i)} - \mu_j) \right) \right]^{\mathbb{I}[y^{(i)} = j]} \tag{7}
$$



### (a) lower bound derivation [3 points]

**(a)** æ¨å¯¼ objective $J$ çš„ variational lower bound . Specifically, è¯æ˜å¯¹äºä»»æ„çš„æ¦‚ç‡åˆ†å¸ƒ $q_i(y^{(i)} = j)$ï¼Œobjective çš„ lower bound å¯ä»¥å†™ä¸ºï¼š
$$
L(\mu, \Sigma, \phi) = \sum_{i=1}^l \log p(x^{(i)}, y^{(i)}) + \lambda \sum_{i=l+1}^{l+u} \sum_{j \in \{0,1\}} Q_{ij} \log \frac{p(x^{(i)}, y^{(i)} = j)}{Q_{ij}} \tag{8}
$$

å…¶ä¸­ï¼Œ$Q_{ij} \triangleq q_i(y^{(i)} = j)$ æ˜¯è®°å·ç®€åŒ–ã€‚

> **æç¤ºï¼š** å¯ä»¥è€ƒè™‘ä½¿ç”¨ Jensen ä¸ç­‰å¼æ¥æ¨å¯¼ã€‚





### (b) E-step [2 points]

Write down the E-step. Specifically, define the distribution $Q_{ij} = q_i(y^{(i)} = j)$



### (c) M-step for $\mu_k$ [6 points]

æ¨å¯¼å½“ $k = 0$ æˆ– $1$ æ—¶ï¼Œ$\mu_k$ çš„ M-step update rule, while holding $Q_i$'s fixed.

å¹¶æ–‡å­—è§£é‡Šï¼šä»ç›´è§‰ä¸Šçœ‹ï¼Œwhat $\mu_k$ looks like ? in terms of $x^{(i)}$'s (labeled and unlabeled) ä»¥åŠ pseudo-counts.



### (d) M-step for $\phi$ 

[6 points] æ¨å¯¼ $\phi \in \mathbb{R}$ çš„ M-step update rule, while holding $Q_i$'s fixed.

å¹¶æ–‡å­—è§£é‡Šï¼šä»ç›´è§‰ä¸Šçœ‹ï¼Œwhat $\phi$ looks like ? in terms of $x^{(i)}$'s (labeled and unlabeled) ä»¥åŠ pseudo-counts.





### (e) M-step for $\Sigma_k$

[3 points] æœ€åï¼Œbased on analogy, å†™å‡º $\Sigma_k$ ï¼ˆ$k = 0$ æˆ– $1$ï¼‰çš„ M-step update ruleã€‚

ç”±äºæˆ‘ä»¬çŸ¥é“æ¨å¯¼è¿‡ç¨‹ä¸ GDAï¼ˆé«˜æ–¯åˆ¤åˆ«åˆ†æï¼‰æˆ– GMM çš„ M æ­¥ç±»ä¼¼ï¼Œå› æ­¤ä½ ä¸éœ€è¦é‡å¤å®Œæ•´æ¨å¯¼æ­¥éª¤ï¼ˆä½ å·²ç»åœ¨ä¹‹å‰ä»»åŠ¡ä¸­ç»ƒä¹ è¿‡ï¼‰ã€‚

å¹¶æ–‡å­—è§£é‡Šï¼šä»ç›´è§‰ä¸Šçœ‹ï¼Œwhat $\Sigma_k$ looks like ? in terms of $x^{(i)}$'s (labeled and unlabeled) ä»¥åŠ pseudo-counts.



