

## Transfer learning and fine-tuning

**Transfer Learning（迁移学习）** 和 **Fine-Tuning（微调）** 都是基于 **预训练模型（Pre-trained models）** 的方法，用于在新的任务上高效地训练深度学习模型，特别是在计算资源有限或数据量不足的情况下。

Transfer learning 的核心思想是使用**已经在大规模数据集（如 ImageNet）上训练好的模型**，然后将其应用到**新的但相似的任务**上，而不需要从零开始训练整个网络。

- **不调整预训练模型的大部分参数**，而是**利用其提取的特征**。
- 一般只**移除最后几层**，替换为新的分类层，并在新数据集上训练新的分类器。
- 适用于**数据量较少**的情况，能够**快速收敛**。

**主要步骤**:

1. **使用预训练模型的权重**（例如 ResNet、VGG、EfficientNet）。
2. **移除最后的分类层**（通常是全连接层）。
3. **使用网络的前面部分作为特征提取器**（冻结这些层的权重）。
4. **添加新的分类头（通常是一个全连接层）** 并在新的任务数据上训练。

**应用场景**

- 计算机视觉：图像分类、目标检测（如从 ImageNet 迁移到医学影像分析）。
- 自然语言处理（NLP）：使用预训练的 BERT 或 GPT 进行情感分析等任务。





**Fine-Tuning（微调）** 是迁移学习的一种**更深入的**方法，不仅仅是利用预训练模型的特征提取能力，还**对部分或全部预训练模型的参数进行调整**，以适应新任务。

1. **去掉预训练模型的最后几层**。
2. **添加新的分类头**，先训练这个部分（类似迁移学习）。
3. **解冻部分深层（或整个网络）的参数**，使用较小的学习率进行训练（防止破坏预训练的权重）。
4. **训练整个网络**，进一步优化新任务上的性能。

适用于任务和预训练数据集的相似度较高时（如猫狗分类 → 狼狗分类）。



| 特性             | 迁移学习 (Transfer Learning)                            | 微调 (Fine-Tuning)                              |
| ---------------- | ------------------------------------------------------- | ----------------------------------------------- |
| **冻结层数**     | 大部分层被冻结，仅训练新分类层                          | 可能解冻部分或全部层进行训练                    |
| **计算资源需求** | 低                                                      | 高                                              |
| **数据需求**     | 少                                                      | 需要更多数据                                    |
| **适用场景**     | 任务不同但特征相似（如 ImageNet 迁移到 X-Ray 影像分类） | 任务相似，需更精确调整（如猫狗分类 → 狼狗分类） |
| **训练时间**     | 较短                                                    | 较长                                            |

